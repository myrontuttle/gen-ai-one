{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ## Install and run ollama\n",
   "id": "ac17cab30ac71a7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T14:51:04.484409Z",
     "start_time": "2024-11-22T14:51:03.382171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "#you need a variable environment with the model name (check the .env.example)\n",
    "os.environ['OLLAMA_MODEL_NAME'] = 'phi3:3.8b-mini-4k-instruct-q4_K_M'\n",
    "\n",
    "!make install-ollama"
   ],
   "id": "6dc762b4ab8fdbd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;33m=========> Installing ollama first...\u001B[0m\r\n",
      "Detected macOS. Installing Ollama with Homebrew...\r\n",
      "^C\r\n",
      "\r\n",
      "make: *** [install-ollama] Error 130\r\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T14:34:44.067469Z",
     "start_time": "2024-11-22T14:34:42.690271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# download a model (default one is in the env variable OLLAMA_MODEL_NAME)\n",
    "!make download-ollama-model"
   ],
   "id": "e3f80472e8f59897",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;33mDownloading local model phi3:3.8b-mini-4k-instruct-q4_K_M ...\u001B[0m\r\n",
      "\u001B[?25l\u001B[?25l\u001B[?25h\u001B[2K\u001B[1G\u001B[?25h\u001B[?2004h>>> \u001B[38;5;245mSend a message (/? for help)\u001B[28D\u001B[0m\u001B[K\r\n",
      "Use Ctrl + d or /bye to exit.\r\n",
      ">>> \u001B[38;5;245mSend a message (/? for help)\u001B[28D\u001B[0m\u001B[K\r\n",
      ">>> \u001B[38;5;245mSend a message (/? for help)\u001B[28D\u001B[0m"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T14:34:45.995237Z",
     "start_time": "2024-11-22T14:34:45.795317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run ollama server in your terminal (process background doesn't work in jupyter)\n",
    "!make run-ollama"
   ],
   "id": "f2dfebe7d6023848",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;33mRunning ollama...\u001B[0m\r\n",
      "Error: listen tcp 127.0.0.1:11434: bind: address already in use\r\n",
      "make: *** [run-ollama] Error 1\r\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Emulating OpenAI with Ollama",
   "id": "4c595d1c71e515df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T14:25:09.923625Z",
     "start_time": "2024-11-22T14:25:09.717810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# install ollama python library (you use it without uv)\n",
    "!uv pip install ollama"
   ],
   "id": "164b8a9e108887c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2mAudited \u001B[1m1 package\u001B[0m \u001B[2min 14ms\u001B[0m\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T23:48:41.898808Z",
     "start_time": "2024-11-22T23:48:40.734448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    # required but ignored\n",
    "    api_key='t',\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'Hi',\n",
    "        }\n",
    "    ],\n",
    "    # same model that you downloaded in ollama\n",
    "    model='phi3:3.8b-mini-4k-instruct-q4_K_M',\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ],
   "id": "4818cad65e45570f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Emulating AzureOpenAI with Ollama\n",
   "id": "530fa68e3828caef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T14:50:52.822075Z",
     "start_time": "2024-11-22T14:50:50.035395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# install ollamazure\n",
    "!make install-ollamazure"
   ],
   "id": "a9f0155e334ad1fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo \"\\033[0;33m=========> Installing Evaluation app \\033[0m\"\r\n",
      "\u001B[0;33m=========> Installing Evaluation app \u001B[0m\r\n",
      "\u001B[0;33mNVM is already installed.\u001B[0m\r\n",
      "Found '/Users/z736kw/Downloads/git/ai-cloud-project-template/.nvmrc' with version <22.8.0>\r\n",
      "Now using node v22.8.0 (npm v10.8.2)\r\n",
      "# Activate NVM (makefile runs in a subshell, always use this)\r\n",
      "\u001B[0;33mRestart your terminal to use nvm.  If you are on MacOS, run nvm ls, if there is no node installed, run nvm install \u001B[0m\r\n",
      "\u001B[0;32m->      v22.8.0\u001B[0m\r\n",
      "\u001B[0;32mdefault\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;32m22.8.0\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;32mv22.8.0\u001B[0m)\r\n",
      "\u001B[0;31miojs\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;31mN/A\u001B[0m \u001B[0;37m(default)\u001B[0m\r\n",
      "\u001B[0;31munstable\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;31mN/A\u001B[0m \u001B[0;37m(default)\u001B[0m\r\n",
      "\u001B[0;32mnode\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;32mstable\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;32mv22.8.0\u001B[0m) \u001B[0;37m(default)\u001B[0m\r\n",
      "\u001B[0;32mstable\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;32m22.8\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;32mv22.8.0\u001B[0m) \u001B[0;37m(default)\u001B[0m\r\n",
      "\u001B[1;33mlts/*\u001B[0m \u001B[0;90m->\u001B[0m \u001B[1;33mlts/jod\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;31mN/A\u001B[0m)\r\n",
      "\u001B[1;33mlts/argon\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;31mv4.9.1\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;31mN/A\u001B[0m)\r\n",
      "\u001B[1;33mlts/boron\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;31mv6.17.1\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;31mN/A\u001B[0m)\r\n",
      "\u001B[1;33mlts/carbon\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;31mv8.17.0\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;31mN/A\u001B[0m)\r\n",
      "\u001B[1;33mlts/dubnium\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;31mv10.24.1\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;31mN/A\u001B[0m)\r\n",
      "\u001B[1;33mlts/erbium\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;31mv12.22.12\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;31mN/A\u001B[0m)\r\n",
      "\u001B[1;33mlts/fermium\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;31mv14.21.3\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;31mN/A\u001B[0m)\r\n",
      "\u001B[1;33mlts/gallium\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;31mv16.20.2\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;31mN/A\u001B[0m)\r\n",
      "\u001B[1;33mlts/hydrogen\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;31mv18.20.5\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;31mN/A\u001B[0m)\r\n",
      "\u001B[1;33mlts/iron\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;31mv20.18.0\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;31mN/A\u001B[0m)\r\n",
      "\u001B[1;33mlts/jod\u001B[0m \u001B[0;90m->\u001B[0m \u001B[0;31mv22.11.0\u001B[0m (\u001B[0;90m->\u001B[0m \u001B[0;31mN/A\u001B[0m)\r\n",
      "Found '/Users/z736kw/Downloads/git/ai-cloud-project-template/.nvmrc' with version <22.8.0>\r\n",
      "Now using node v22.8.0 (npm v10.8.2)\r\n",
      "^C\r\n",
      "make: *** [install-nvm] Interrupt: 2\r\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T14:40:14.055582Z",
     "start_time": "2024-11-22T14:40:14.046180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run ollamazure in your terminal (process background doesn't work in jupyter)\n",
    "!make run-ollamazure"
   ],
   "id": "71e96be57a139984",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Background processes not supported.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# run ollamazure\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msystem\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmake run-ollamazure &&\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Downloads/git/ai-cloud-project-template/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py:641\u001B[0m, in \u001B[0;36mZMQInteractiveShell.system_piped\u001B[0;34m(self, cmd)\u001B[0m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cmd\u001B[38;5;241m.\u001B[39mrstrip()\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m&\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    635\u001B[0m     \u001B[38;5;66;03m# this is *far* from a rigorous test\u001B[39;00m\n\u001B[1;32m    636\u001B[0m     \u001B[38;5;66;03m# We do not support backgrounding processes because we either use\u001B[39;00m\n\u001B[1;32m    637\u001B[0m     \u001B[38;5;66;03m# pexpect or pipes to read from.  Users can always just call\u001B[39;00m\n\u001B[1;32m    638\u001B[0m     \u001B[38;5;66;03m# os.system() or use ip.system=ip.system_raw\u001B[39;00m\n\u001B[1;32m    639\u001B[0m     \u001B[38;5;66;03m# if they really want a background process.\u001B[39;00m\n\u001B[1;32m    640\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBackground processes not supported.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 641\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(msg)\n\u001B[1;32m    643\u001B[0m \u001B[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001B[39;00m\n\u001B[1;32m    644\u001B[0m \u001B[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001B[39;00m\n\u001B[1;32m    645\u001B[0m \u001B[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001B[39;00m\n\u001B[1;32m    646\u001B[0m \u001B[38;5;66;03m# Also, protect system call from UNC paths on Windows here too\u001B[39;00m\n\u001B[1;32m    647\u001B[0m \u001B[38;5;66;03m# as is done in InteractiveShell.system_raw\u001B[39;00m\n\u001B[1;32m    648\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mplatform \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwin32\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mOSError\u001B[0m: Background processes not supported."
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T23:49:02.263654Z",
     "start_time": "2024-11-22T23:49:01.038956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key='t',\n",
    "    api_version=\"2024-10-01-preview\",\n",
    "    azure_endpoint=\"http://localhost:4041\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'Hi',\n",
    "        }\n",
    "    ],\n",
    "    # same model that you downloaded in ollama\n",
    "    model='phi3:3.8b-mini-4k-instruct-q4_K_M',\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ],
   "id": "bf836b7b10d551e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3fe08eef608b9109"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
