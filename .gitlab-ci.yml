default:
  image: aminedjeghri/python-uv-node:latest

stages:
  - evaluate


variables:
  VENV_PATH: "$CI_PROJECT_DIR/.venv"  # Define virtual environment path
  OLLAMA_MODEL_NAME: "phi3:3.8b-mini-4k-instruct-q4_K_M"
  OLLAMA_EMBEDDING_MODEL_NAME: "all-minilm:l6-v2"
  LLM_PROVIDER: "openai"
  OPENAI_DEPLOYMENT_NAME: "phi3:3.8b-mini-4k-instruct-q4_K_M" # or gpt-4o-mini if you use openai
  OPENAI_BASE_URL: "http://localhost:11434/v1" # ollama endpoint or https://api.openai.com/v1 if use use openai
  OPENAI_API_KEY: "t"

  ENABLE_EVALUATION: true # if true, you need to set the following
  # LLMAAJ stands for LLM as a judge
  LLMAAJ_PROVIDER: "openai" # or azure_openai
  #if openai
  LLMAAJ_OPENAI_DEPLOYMENT_NAME: "phi3:3.8b-mini-4k-instruct-q4_K_M" # or gpt-4o-mini if you use openai
  LLMAAJ_OPENAI_BASE_URL: "http://localhost:11434/v1" # ollama endpoint or https://api.openai.com/v1 if use use openai
  LLMAAJ_OPENAI_API_KEY: "t"
  LLMAAJ_OPENAI_EMBEDDING_DEPLOYMENT_NAME: "text-embedding-ada-002"

cache:
  key:
    files: # can't use more than two files in a single cache key.
      - pyproject.toml #or if you have a folder /requirements/*
      - package.json

  paths:
    - node_modules/
    - $VENV_PATH  # Cache the virtual environment

evaluate:
  stage: evaluate  # Reference the correct stage
  script:
    - if [ ! -d node_modules ]; then make install-npm-dependencies; fi
    - |
      if [ ! -d $VENV_PATH ]; then
        echo "Virtual environment does not exist. Creating..."
        make install-dev
      fi

    # Run evaluation
    - make download-ollama-model
    - ollama serve && make test
